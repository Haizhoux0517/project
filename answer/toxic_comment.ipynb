{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spellchk: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellchk(fh):\n",
    "    for (locations, sent) in get_typo_locations(fh):\n",
    "        spellchk_sent = sent\n",
    "        sent_lower = [word.lower() for word in sent]\n",
    "        for i in locations:\n",
    "            # predict top_k replacements only for the typo word at index i\n",
    "            predict = fill_mask(\n",
    "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
    "                top_k=200\n",
    "            )\n",
    "            predict = [pred['token_str'] for pred in predict]\n",
    "            logging.info((sent_lower[i], predict))\n",
    "            spellchk_sent[i] = select_correction(sent_lower[i], predict)\n",
    "        yield(locations, spellchk_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    return predict[0]['token_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spellchk funtion takes in the typo word from the given location in the sentence. Then it predicts the possible replacement for that typo word using the Hugging Face model, with the 1st word in the dictionary being the most possible replacement for that typo word. Then the predicted dictionary of possible replacement words is sent to the select_correction function, where we choose the correct possible word to replace the typo word. By default, it will choose the first word in the dictionary, since it has the highest score among the other possible word replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_rep(word):\n",
    "    chrep = np.zeros((1, 128))\n",
    "    for c in word:\n",
    "        chrep[0][ord(c)] += 1\n",
    "    return chrep\n",
    "\n",
    "def select_correction(typo, predict):\n",
    "    # remove predictions that contain non-ASCII characters\n",
    "    predict = [word for word in predict if all(ord(c) < 128 for c in word)]\n",
    "\n",
    "    # find the counts of each distinct character in the typo\n",
    "    typo_char = char_rep(typo)\n",
    "    predict_chars = np.vstack([char_rep(word) for word in predict])\n",
    "    \n",
    "    # return prediction with most similar counts\n",
    "    differences = np.linalg.norm(predict_chars - typo_char, axis=1)\n",
    "    return predict[np.argmin(differences)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created character level representation for the predictions and typo. Each word is represented by a 128-dimensional numpy array that contains the counts of distinct ascii characters. We removed all predictions that contain non-ascii characters and made typo lowercase beforehand. If we are considering N predictions from the Hugging Face model, the predictions are represented by is a N x 128 matrix while the typo is represented by a 1 x 128 row vector. Then we compute the N x 128 matrix that contains the difference between each prediction and typo. We obtain the row index where the norm is lowest, which gives us the best prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experimented with different number of predictions being considered to substitute typo. We tried N in {10, 20, 50, 100, 200, 500, 1000}. N = 200 gave us the best result with dev.score = 0.81. We also tried substituting L1 with L2 norm to for efficiency but observed no noticeable speedup. \n",
    "\n",
    "We also tried representing each word with a different character level representation: a 130-dimensional array with first 128 entries containing the counts of distinct ascii characters (excluding the first and last character) and 129th and 130th entry containing first and last character's ascii integer representations. The result was worse than representing each word with a 128-dimensional array that contains the counts of distinct ascii characters (including the first and last character)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone worked together and communicated well on the discord to finish this homework. Everyone contributed to both the coding and the notebook part by adding their work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
